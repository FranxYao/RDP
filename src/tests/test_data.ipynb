{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suburban-spain",
   "metadata": {},
   "source": [
    "# Test datasets for BertNet\n",
    "\n",
    "Yao Fu. University of Edinburgh<br />\n",
    "yao.fu@ed.ac.uk<br />\n",
    "Jun 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-kelly",
   "metadata": {},
   "source": [
    "# 20 News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "wrapped-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from transformers import BertTokenizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-quantity",
   "metadata": {},
   "source": [
    "# Use Spacy to process document into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "preceding-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "following-ceremony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fd6fd696e60>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fd6fd522f68>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fd6fd2eaad8>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fd6fd2eac10>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7fd6fd560848>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fd6fd54b1c8>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tired-thousand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7fd6fd444ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "independent-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='all')['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "grateful-virus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18846it [04:51, 64.75it/s] \n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for doc in tqdm(nlp.pipe(data, disable=['tok2vec', 'parser', 'ner'])):\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "effective-isolation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Does anyone have suggestions/ideas on:\n",
       "\n",
       "  - Diamond Stealth Pro Local Bus\n",
       "\n",
       "  - Orchid Farenheit 1280\n",
       "\n",
       "  - ATI Graphics Ultra Pro\n",
       "\n",
       "  - Any other high-performance VLB card\n",
       "\n",
       "\n",
       "Please post or email."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(docs[1].sents)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pregnant-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "outside-humidity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] does anyone have suggestions / ideas on : - diamond stealth pro local bus - orchid fare ##nh ##eit 128 ##0 - at ##i graphics ultra pro - any other high - performance v ##lb card please post or email . [SEP]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer.convert_ids_to_tokens(tokenizer(str(list(docs[1].sents)[2]))['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "improved-nightlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [03:03<00:00, 102.45it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = []\n",
    "for d in tqdm(docs):\n",
    "    for s in list(d.sents):\n",
    "        tokens = tokenizer(str(s))['input_ids']\n",
    "        tokenized_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "french-asthma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] from : mama ##tha devin ##eni rat ##nam < mr ##47 + @ andrew . cm ##u . ed ##u > subject : pens fans reactions organization : post office , carnegie mellon , pittsburgh , pa lines : 12 n ##nt ##p - posting - host : po ##4 . andrew . cm ##u . ed ##u i am sure some bash ##ers of pens fans are pretty confused about the lack of any kind of posts about the recent pens massacre of the devils . [SEP]'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer.convert_ids_to_tokens(tokenized_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "electronic-affect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"then you can use asynch sndplay's all you want.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_sentences[1000][1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-peoples",
   "metadata": {},
   "source": [
    "# Tokenized Sentences Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "signal-relevance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311862"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-ecology",
   "metadata": {},
   "source": [
    "## Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "executed-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for s in tokenized_sentences:\n",
    "    lens.append(len(s))\n",
    "lens = np.array(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "noticed-foundation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11004425, 5547067)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.sum(), lens[lens < 50].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "developing-tennessee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311862, 265640)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lens), (lens < 50).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-links",
   "metadata": {},
   "source": [
    "## Effective Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "desperate-regression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25807\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for s in tokenized_sentences:\n",
    "    vocab.extend(s)\n",
    "vocab = Counter(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-killer",
   "metadata": {},
   "source": [
    "# Store sentences into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "answering-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311862/311862 [00:19<00:00, 15684.28it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/news/20news.txt', 'w', encoding='utf-8') as fd:\n",
    "    for s in tqdm(tokenized_sentences):\n",
    "        slen = len(s)\n",
    "        if(slen < 50 and slen > 2):\n",
    "            s_ = tokenizer.decode(s[1:-1]).replace('*', '').replace('>', '').replace('-', '').replace('<', '').replace('|', '').replace('^', '')\n",
    "            s_ = ' '.join(s_.split()).strip()\n",
    "            fd.write(s_)\n",
    "            fd.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "breeding-youth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2941, 1010, 1045, 2572, 2978, 14909, 2205, 1998, 1037, 2978, 7653, 1012, 102], [101, 2174, 1010, 1045, 2572, 2183, 2000, 2404, 2019, 2203, 2000, 2512, 6278, 2545, 1005, 4335, 2007, 1037, 2978, 1997, 8489, 2005, 1996, 25636, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(lines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efficient-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_cnt = Counter(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "broke-substance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1007])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(data[0:5], padding=True, return_tensors='pt')['input_ids'].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
